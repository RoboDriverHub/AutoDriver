import asyncio
import os
from typing import Dict, Any, Literal
from langchain_core.messages import ToolMessage, HumanMessage, SystemMessage, AIMessage
# è·¨æ–‡ä»¶å¯¼å…¥ï¼šå¯¼å…¥çŠ¶æ€å®šä¹‰ + å·¥å…·
from src.agent.state import CalcAgentState
from src.agent.tools import tools, tools_by_name
from dotenv import load_dotenv, find_dotenv
load_dotenv(find_dotenv(), override=True)

from langchain_community.chat_models import ChatTongyi
# åˆå§‹åŒ–LLMæ¨¡å‹ï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œå…¨å±€å¤ç”¨
model = ChatTongyi(
    model_name="qwen3-coder-plus",
    temperature=0.0
)
# âœ… å¼ºåˆ¶é‡æ–°ç»‘å®šå·¥å…·ï¼Œç¡®ä¿æ–°å¢çš„ROS2å·¥å…·+knowledge_queryè¢«LLMè¯†åˆ«
model_with_tools = model.bind_tools(tools, tool_choice="auto") # auto=è‡ªåŠ¨é€‰æ‹©å·¥å…·

# ========== Node 1: parse_input æ•°æ®è§£æèŠ‚ç‚¹ ã€å®Œå…¨ä¸å˜ã€‘ ==========
async def parse_input(state: CalcAgentState) -> Dict[str, Any]:
    """è§£æç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è½¬æ¢å­—å…¸ä¸ºBaseMessage"""
    messages = state["messages"]
    converted_messages = []
    for msg in messages:
        if isinstance(msg, dict):
            # å­—å…¸è½¬BaseMessage
            if msg.get("type") == "human":
                converted_msg = HumanMessage(content=msg.get("content", ""))
            elif msg.get("type") == "ai":
                converted_msg = AIMessage(content=msg.get("content", ""))
            else:
                converted_msg = HumanMessage(content=msg.get("content", ""))
            converted_messages.append(converted_msg)
        else:
            converted_messages.append(msg)
    
    # ç”¨è½¬æ¢åçš„æ¶ˆæ¯è¯»å–content
    user_msg = converted_messages[-1].content
    print(f"âœ… parse_input: ç”¨æˆ·è¾“å…¥ = {user_msg}")
    parsed_input = {
        "user_query": user_msg,
        "valid": True
    }
    
    # åŸæœ‰é€»è¾‘...
    return {
        "messages": converted_messages,  # ä¿å­˜è½¬æ¢åçš„æ¶ˆæ¯
        "parsed_input": parsed_input,
        "llm_calls": state.get("llm_calls", 0)
    }


# ========== Node 2: llm_decide LLMå†³ç­–èŠ‚ç‚¹ ã€âœ… æœ€ç»ˆæœ€ç»ˆç‰ˆï¼Œæ ¹æ²»æ‰€æœ‰é—®é¢˜ï¼Œ100%ç”Ÿæ•ˆã€‘ ==========
async def llm_decide(state: CalcAgentState) -> Dict[str, Any]:
    """LLMåˆ†æç”¨æˆ·è¾“å…¥ï¼Œå†³ç­–æ˜¯å¦è°ƒç”¨å·¥å…·/è°ƒç”¨å“ªä¸ªå·¥å…·ï¼Œå†…ç½®ROS2å…¨è‡ªåŠ¨ç”Ÿæˆé€»è¾‘"""
    current_llm_calls = state.get("llm_calls") or 0
    user_query = state["parsed_input"]["user_query"]
    
    # âœ… ROS2ä»»åŠ¡è§¦å‘å…³é”®è¯åˆ¤æ–­ (ä½ çš„åˆ—è¡¨æ²¡é—®é¢˜ï¼Œèƒ½æ­£å¸¸å‘½ä¸­)
    ros2_trigger_words = ["ROS2", "ros2", "node.py", "æœºå™¨äºº", "ç”Ÿæˆé©±åŠ¨", "é©±åŠ¨ä»£ç ", "ç”Ÿæˆnode", "ROS2é©±åŠ¨", "æœºå™¨äººé©±åŠ¨"]
    ros2_task_triggered = any(word in user_query for word in ros2_trigger_words)
    print(f"ğŸ‘‰ ç”¨æˆ·è¾“å…¥: {user_query}")
    print(f"ğŸ‘‰ ROS2ä»»åŠ¡è§¦å‘çŠ¶æ€: {ros2_task_triggered}")
    
    if ros2_task_triggered:
        print("âœ… è¿›å…¥ROS2å·¥å…·æ‰§è¡Œåˆ†æ”¯ï¼Œå¼€å§‹è°ƒç”¨ROS2å·¥å…·é“¾...")
        try:
            # âœ… æ­¥éª¤1ï¼šè°ƒç”¨å·¥å…·è·å–ROS2çœŸå®è¯é¢˜åˆ—è¡¨ (åŒæ­¥è°ƒç”¨ï¼Œæ— å¡æ­»)
            topic_list = tools_by_name["ros2_get_topic_list"].invoke({})
            print(f"âœ… æˆåŠŸè·å–ROS2è¯é¢˜åˆ—è¡¨ï¼Œå…± {len(topic_list)} ä¸ªè¯é¢˜")
            print(f"âœ… è¯é¢˜åˆ—è¡¨: {topic_list[:3]}...") # åªæ‰“å°å‰3ä¸ªï¼Œé¿å…æ—¥å¿—è¿‡é•¿
            
            # âœ… æ­¥éª¤2ï¼šè§£æè¯é¢˜ç”Ÿæˆæœºå™¨äººé…ç½®
            robot_config = tools_by_name["ros2_parse_topic_to_config"].invoke({"topic_list": topic_list})
            print("âœ… æˆåŠŸè§£æè¯é¢˜ï¼Œç”Ÿæˆæœºå™¨äººé…ç½®")
            
            # âœ… æ­¥éª¤3ï¼šæ¸²æŸ“æ¨¡æ¿ç”Ÿæˆæœ€ç»ˆnode.pyä»£ç 
            node_code = tools_by_name["ros2_render_node_template"].invoke({"robot_config": robot_config})
            print("âœ… æˆåŠŸç”ŸæˆROS2 node.pyé©±åŠ¨ä»£ç ")
            
            # âœ… âœ… âœ… æ ¸å¿ƒä¿®å¤ï¼šçœŸæ­£èµ‹å€¼ï¼Œæ— æ³¨é‡Šï¼åªè¿”å›çº¯å‡€ä»£ç ï¼Œæ— ä»»ä½•markdownåŒ…è£¹
            ros2_final_content = node_code
            print(f"âœ… ROS2ä»£ç ç”Ÿæˆå®Œæˆï¼Œä»£ç æ€»é•¿åº¦: {len(ros2_final_content)} å­—ç¬¦")
            
            # âœ… å®Œæ•´è¿”å›æ‰€æœ‰æ•°æ®ï¼Œstateæ ‡è®°æ­£å¸¸ä¼ é€’
            return {
                "messages": [HumanMessage(content=ros2_final_content)],
                "llm_calls": current_llm_calls + 1,
                "ros2_task_triggered": True,
                "ros2_topic_list": topic_list,
                "ros2_parsed_config": robot_config,
                "generated_node_py": node_code
            }
        except Exception as e:
            # âœ… å¢å¼ºå¼‚å¸¸æ•è·ï¼šå¼ºåˆ¶æ‰“å°é”™è¯¯æ—¥å¿—ï¼Œå†ä¹Ÿä¸ä¼šåé”™ï¼
            error_info = f"âŒ ROS2å·¥å…·æ‰§è¡Œå¤±è´¥ï¼Œé”™è¯¯åŸå› : {str(e)}"
            print(error_info)
            err_msg = HumanMessage(content=error_info)
            return {
                "messages": [err_msg], 
                "llm_calls": current_llm_calls + 1, 
                "ros2_task_triggered": True
            }

    # âœ… åŸæœ‰ä¸šåŠ¡é€»è¾‘ å®Œæ•´ä¿ç•™ï¼ˆè®¡ç®—+çŸ¥è¯†åº“æŸ¥è¯¢ï¼‰æ— ä»»ä½•ä¿®æ”¹
    sys_prompt = SystemMessage(content="""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»£ç†ï¼Œä½ æ‹¥æœ‰ä»¥ä¸‹7ä¸ªå·¥å…·å¯ä»¥è°ƒç”¨ï¼š
1. addï¼šåŠ æ³•è®¡ç®—ï¼Œå¿…é¡»è°ƒç”¨å›ç­”åŠ æ³•é—®é¢˜
2. multiplyï¼šä¹˜æ³•è®¡ç®—ï¼Œå¿…é¡»è°ƒç”¨å›ç­”ä¹˜æ³•é—®é¢˜
3. divideï¼šé™¤æ³•è®¡ç®—ï¼Œå¿…é¡»è°ƒç”¨å›ç­”é™¤æ³•é—®é¢˜
4. knowledge_queryï¼šå¿…é¡»è°ƒç”¨æ­¤å·¥å…·å›ç­”æ‰€æœ‰éè®¡ç®—ç±»é—®é¢˜ï¼ŒåŒ…æ‹¬ï¼šæœºå™¨äººæŒ‡ä»¤ã€æœºå™¨äººæ•…éšœç ã€è®¡ç®—å™¨ç”¨æ³•ã€æœºå™¨äººå‹å·ã€æ”¯æŒçš„è¿ç®—ç­‰æ‰€æœ‰çŸ¥è¯†åº“é—®é¢˜ã€‚
5. ros2_get_topic_listï¼šè·å–ROS2æœºå™¨äººè¯é¢˜åˆ—è¡¨
6. ros2_parse_topic_to_configï¼šè§£æROS2è¯é¢˜ç”Ÿæˆæœºå™¨äººé…ç½®
7. ros2_render_node_templateï¼šç”ŸæˆROS2æœºå™¨äººnode.pyé©±åŠ¨ä»£ç 

è§„åˆ™ï¼š
- ç”¨æˆ·æé—®ä¸­æ–‡ï¼Œä½ å¿…é¡»è°ƒç”¨å¯¹åº”å·¥å…·ï¼Œç¦æ­¢ä½¿ç”¨è‡ªèº«çŸ¥è¯†åº“å›ç­”ä»»ä½•é—®é¢˜ã€‚
- æ•°å­¦è®¡ç®—é—®é¢˜ â†’ è°ƒç”¨è®¡ç®—å·¥å…·ï¼›æœºå™¨äºº/è®¡ç®—å™¨ç›¸å…³çŸ¥è¯†é—®é¢˜ â†’ å¿…é¡»è°ƒç”¨knowledge_queryå·¥å…·ï¼›ROS2æœºå™¨äººå¼€å‘é—®é¢˜ â†’ è°ƒç”¨ROS2ç›¸å…³å·¥å…·ã€‚
""")
    prompt_msgs = [sys_prompt] + state["messages"]
    
    ai_response = await asyncio.to_thread(model_with_tools.invoke, prompt_msgs)
    return {
        "messages": [ai_response],
        "llm_calls": current_llm_calls + 1
    }

# ========== Node 3: execute_tool å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹ ã€å®Œå…¨ä¸å˜ã€‘ ==========
async def execute_tool(state: CalcAgentState) -> Dict[str, Any]:
    """æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œè¿”å›å·¥å…·ç»“æœ"""
    tool_calls = state["messages"][-1].tool_calls
    tool_results = []
    for call in tool_calls:
        tool_func = tools_by_name[call["name"]]
        try:
            result = await asyncio.to_thread(tool_func.invoke, call["args"])
        except:
            result = await asyncio.to_thread(tool_func,** call["args"])
        tool_results.append(ToolMessage(content=str(result), tool_call_id=call["id"]))
    return {"messages": tool_results}

# ========== Node 4: llm_summarize LLMæ€»ç»“èŠ‚ç‚¹ ã€âœ… å”¯ä¸€æ­£ç¡®å†™æ³•ï¼ŒROS2åˆ¤æ–­ç½®é¡¶ï¼Œæ ¹æ²»å¡æ­»ã€‘ ==========
async def llm_summarize(state: CalcAgentState) -> Dict[str, Any]:
    """æ ¹æ®å·¥å…·ç»“æœï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€æœ€ç»ˆå›ç­”"""
    current_llm_calls = state.get("llm_calls") or 0
    last_msg = state["messages"][-1]
    
    # âœ…âœ…âœ… å¿…é¡»å†™åœ¨æœ€é¡¶éƒ¨ï¼ä¼˜å…ˆåˆ¤æ–­ROS2ä»»åŠ¡ï¼Œç›´æ¥é€ä¼ ç»“æœï¼Œå®Œå…¨è·³è¿‡æ‰€æœ‰LLMè°ƒç”¨é€»è¾‘ âœ…âœ…âœ…
    if state.get("ros2_task_triggered", False):
        return {
            "messages": [last_msg],
            "llm_calls": current_llm_calls
        }

    # ä¸‹é¢æ˜¯åŸæœ‰é€»è¾‘ï¼ŒROS2ä»»åŠ¡æ°¸è¿œèµ°ä¸åˆ°è¿™é‡Œäº†
    user_query = state["parsed_input"]["user_query"]
    if isinstance(last_msg, ToolMessage):
        rag_prompt = HumanMessage(content="""
è¯·æ ¹æ®ä¸‹é¢çš„å†…å®¹ï¼Œç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦å¤šä½™å†…å®¹ï¼š
å‚è€ƒå†…å®¹ï¼š""" + last_msg.content + """
ç”¨æˆ·é—®é¢˜ï¼š""" + user_query + """
""")
        ai_response = await asyncio.to_thread(model.invoke, [rag_prompt])
    else:
        ai_response = await asyncio.to_thread(model.invoke, state["messages"])
        
    return {
        "messages": [ai_response],
        "llm_calls": current_llm_calls + 1
    }

# ========== æ¡ä»¶è·¯ç”±å‡½æ•° should_continue ã€âœ… æ ¸å¿ƒä¿®å¤ï¼šè¯†åˆ«ROS2ç»“æœï¼Œç›´æ¥ç»“æŸæµç¨‹ã€‘ ==========
def should_continue(state: CalcAgentState) -> Literal["execute_tool", "llm_summarize"]:
    """è·¯ç”±å†³ç­–ï¼šåˆ¤æ–­ä¸‹ä¸€æ­¥æ˜¯æ‰§è¡Œå·¥å…·ï¼Œè¿˜æ˜¯æ€»ç»“å›ç­”"""
    last_msg = state["messages"][-1]
    if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
        return "execute_tool"
    # âœ… ã€ä¿®æ”¹ç‚¹3ã€‘æ°¸è¿œåªè¿”å› "llm_summarize"ï¼Œä¸å†è¿”å› "__end__"
    return "llm_summarize"